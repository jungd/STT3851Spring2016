\documentclass[final]{beamer}
\usefonttheme{serif}
\mode<presentation>{\usetheme{ASU}}
\usepackage{amsmath, amsfonts, amssymb, pxfonts, eulervm, xspace, enumerate, hyperref, color, bookmark}
\usepackage{graphicx}
\usepackage[orientation=landscape, size=a0, scale=1.4, debug]{beamerposter}
% \usepackage{natbib}

\usecolortheme{rose}
%\setbeamercolor{background canvas}{bg=magenta!16!yellow!90}

\beamertemplategridbackground[1cm]

%-- Header and footer information ----------------------------------
\newcommand{\footright}{\href{https://github.com/STAT-ATA-ASU/STT3851Spring2016}{https://github.com/STAT-ATA-ASU/STT3851Spring2016}}
\newcommand{\footleft}{\href{mailto:arnholtat@appstate.edu}{Faculty Advisor: Alan Arnholt}}

\def\conference{King County House Price Prediction Project}
\title{Your Descriptive Title Here}
\author{FirstName1 LastName1, FirstName2 LastName2, FirstName3 LastName3} 
\institute{Department of Mathematical Sciences}
%-------------------------------------------------------------------


%-- Main Document --------------------------------------------------
\begin{document}
\begin{frame}[fragile]
\vspace{-2ex}
\begin{columns}[t]

<<label = "setup", comment = NA, echo = FALSE, message = FALSE, warning=FALSE>>=
options(width = 35)
opts_chunk$set(comment = NA, fig.align='center', fig.width=13, fig.height=6, cache=FALSE, warning = FALSE, message = FALSE)
library(maps)
library(ggplot2)
library(PASWR)
library(tree)
library(randomForest)
library(boot)
library(xtable)
library(ggmap)
housedata <- read.csv("../Data/housedata.csv", 
                      colClasses = c(id = "character", date = "character", 
                                     yr_built = "character", zipcode = "factor", grade = "factor"))
housedata$date <- as.Date(housedata$date, "%Y%m%d")
housedata$waterfront <- factor(housedata$waterfront, labels = c("No", "Yes"))
housedata$condition <- factor(housedata$condition, labels = c("poor", "fair", "average", "good", "very good"))
housedata$yr_renovated <- ifelse(housedata$yr_renovated == 0, housedata$yr_built, housedata$yr_renovated)
housedata$yr_built <- as.Date(ISOdate(housedata$yr_built, 9, 1))  # Complete Year, Sept 1
housedata$yr_renovated <- as.Date(ISOdate(housedata$yr_renovated, 9, 1))  # Last renovated Year, Sept 1
housedata <- housedata[, -1]
housedataT <- read.csv("../Data/housedataTEST.csv",
                      colClasses = c(id = "character", date = "character", 
                                     yr_built = "character", zipcode = "factor", grade = "factor"))
housedataT$date <- as.Date(housedataT$date, "%Y%m%d")
housedataT$waterfront <- factor(housedataT$waterfront, labels = c("No", "Yes"))
housedataT$condition <- factor(housedataT$condition, labels = c("poor", "fair", "average", "good", "very good"))
housedataT$yr_renovated <- ifelse(housedataT$yr_renovated == 0, housedataT$yr_built, housedataT$yr_renovated)
housedataT$yr_built <- as.Date(ISOdate(housedataT$yr_built, 9, 1))  # Complete Year, Sept 1
housedataT$yr_renovated <- as.Date(ISOdate(housedataT$yr_renovated, 9, 1))  # Last renovated Year, Sept 1
housedataT <- housedataT[, -1]
@

%-- Column 1 ---------------------------------------------------
\begin{column}{0.31\linewidth}
\begin{minipage}[t][.955\textheight]{\linewidth} 
%-- Block 1-1
% \vspace{0ex}
\begin{block}{Overview}
\begin{itemize}
\item On May 8, 2012, North Carolina voters approved Amendment One.  This poster examines four different models used to predict North Carolina county voting behavior.  
\item To ensure accurate predictive power for future observations, the data are split into a training set (80\%) and a test set (20\%). 
\item Root mean squared error of the test set is used as a measure of model adequacy.  
\item All computations and graphs are created with the open source software \texttt{R} \cite{R-base}. 
\end{itemize}
\vspace{0ex}
\end{block}
\vfill

%-- Block 1-2
\begin{block}{K-Fold Cross-Validation}
\begin{itemize}
\item Cross validation is the simplest and most widely used method for estimating prediction error \cite{JF09}.  This method directly estimates the expected extra-sample error, $Err = E[{L(Y, \hat{f\,}\!(X))}]$.  In this work, the loss function, $L$, is the square root of the average squared error loss.
\vspace{2ex}
\item The data in this project is split into $K=10$ equal sized parts.  The cross-validation estimate of the prediction error is $$CV(\hat{f\,}\!)=\frac{1}{N}\sum_{i=1}^{N}L(y_i, \hat{f\,}\!^{-K(i)}(x_i)),$$
where $\hat{f\,}\!^{-K}(x)$ denotes the fitted function with the $K$\textsuperscript{th} part of the data removed.
\end{itemize}
\vspace{0ex}
\vfill
\end{block}
\vfill

%-- Block 1-3
\begin{block}{Basic Models Used}
\begin{enumerate}[I.]
\item Least Squares Regression
\vspace{2ex}

Note: Variables are described in the Variable Table handout.
\begin{enumerate}[a.]
\item Model from \cite{DE12} applied to Training data (\textcolor{blue}{mod1A}) --- Percent voting for Amendment One is modeled using the predictors pct18.24, medinc, pctb, mccain08, evanrate, pctrural, and pctba. 
\item Our OLS model applied to Training data (\textcolor{blue}{mod1B}) --- Percent voting for Amendment One is modeled using the predictors obama08, pctrural, pctw, pctd, pctb, log(pct18.24), log(pctcolenrol), pctfm, log(pctfd), pctown,  medinc, $\text{medinc}^2$, evanrate, $\text{evanrate}^2$, pctfb, $\text{pctfb}^2$, log(pctstud), and log(colden).
\end{enumerate}
\item Cross validated, $K = 10$, and pruned, $n_{\text{leaves}}=4$, regression tree \cite{JF09} (\textcolor{blue}{mod2})
\item Random Forest built from, $n_{\text{trees}} = 5000 $, \cite{AL02} (\textcolor{blue}{mod3})
\end{enumerate}
\vspace{0ex}
<<REGmodels, echo = FALSE>>=
NCV <- read.csv("../Data/NCV03-29-13.csv")
set.seed(13)
train_index <- sample(1:100,size=80)
train1_NCV <- NCV[train_index,]
test1_NCV <- NCV[-train_index,]
train_NCV <- train1_NCV[,-c(1,6,7,11,13,14,15,16,17,18,
                           19,20,33,34,35,36,37,38,39,40,49,50)]
test_NCV <- test1_NCV[,-c(1,6,7,11,13,14,15,16,17,18,
                           19,20,33,34,35,36,37,38,39,40,49,50)]
BJmod <- glm(pctfor ~ pct18.24 + medinc + pctb + mccain08 + evanrate + pctrural + pctba, data = train_NCV, family = gaussian)
# summary(BJmod)
# removed some variables...some more to consider???:  medinc + I(medinc^2) + evanrate + I(evanrate^2) + pctfb + I(pctfb^2) + log(pctstud + 1e-6)  + log(colden + 1e-6)
EMmod <- glm(pctfor ~ obama08 + pctrural + pctw + pctd + pctb + log(pct18.24 + 1e-6)  + log(pctcolenrol + 1e-6) + pctfm + log(pctfd + 1e-6) + pctown + medinc + I(medinc^2) + evanrate + I(evanrate^2) + pctfb + I(pctfb^2) + log(pctstud + 1e-6)  + log(colden + 1e-6) , data=train_NCV, family = gaussian)

RMSEmod1aNoCV <- sqrt(mean(resid(BJmod)^2))
# cross-validated
val.10.fold <- cv.glm(data = train_NCV, glmfit = BJmod, K = 10)
RMSEmod1a <- val.10.fold$delta[1]^.5
val.10.foldEM <- cv.glm(data = train_NCV, glmfit = EMmod, K = 10)
RMSEmod1b <- val.10.foldEM$delta[1]^.5
@
\end{block}
\vfill

\end{minipage}
\end{column}%1

%-- Column 2 ---------------------------------------------------

\begin{column}{0.31\linewidth}
\begin{minipage}[t][.955\textheight]{\linewidth} 
%-- Block 2-1
%\vspace{0ex}
\begin{block}{Cross Validated and Pruned Tree}
%\vspace{0ex}
<<TREE, echo = FALSE, fig.width = 12, fig.height= 12>>=	  
options(digits = 7)
set.seed(21)
ncTree <- tree(pctfor ~ . , data = train_NCV)
# plot(cv.tree(ncTree, FUN = prune.tree, method = "deviance"))
pruneTree <- prune.tree(ncTree, best = 4)
plot(pruneTree)
text(pruneTree, cex = 1.5)
RMSEmod2 <- sqrt(mean(resid(pruneTree)^2))
rfMod <- randomForest(pctfor ~., data = train_NCV, ntree = 5000, importance = TRUE, na.action = na.omit, proximity = TRUE)
# print(rfMod)
NCV$RFpctfor <- predict(rfMod, newdata = NCV)
NCV$EMWpctfor <- predict(EMmod, newdata = NCV)
@
\vspace{-3ex}
\end{block}
\vfill

%-- Block 2-2
%\vspace{3ex}
\begin{block}{King County Real Estate}
\vspace{0ex}
<<KCmap, echo = FALSE, fig.width = 12, fig.height = 12>>=
KingMap <- get_map(location = c(lon = -122.1, lat = 47.48), zoom = 10, source = "google", maptype = "roadmap")
ggmap(KingMap) +
  geom_point(aes(x = long, y = lat, color = price), data = housedata,
 alpha = .2, size = .05) + scale_color_gradient(low = "red", high = "black",limits = c(70000, 8000000)) +
  ggtitle("Houses Sold in King County, Wa (2014-2015)") + 
  labs(x = "longitude", y = "latitude")
@
\vspace{0ex}
\end{block}
\vfill

\end{minipage}
\end{column}%2

%-- Column 3 ---------------------------------------------------
\begin{column}{0.31\linewidth}
\begin{minipage}[t][.955\textheight]{\linewidth} 
%-- Block 3-1
\vspace{0ex}
\begin{block}{Random Forest Variable Importance}
\vspace{-6ex}
<<Rforest, echo = FALSE, fig.height=6.5>>=
varImpPlot(rfMod, main = "", n.var = 10, pch = 19, cex=1.4, col = "black")
RMSEmod3 <- sqrt(mean(rfMod$mse))
###########
RMSEtestmod1a <- sqrt(mean((predict(BJmod, newdata= test_NCV) - test_NCV$pctfor)^2))
RMSEtestmod1b <- sqrt(mean((predict(EMmod, newdata= test_NCV) - test_NCV$pctfor)^2))
RMSEtestmod2 <- sqrt(mean((predict(pruneTree, newdata= test_NCV) - test_NCV$pctfor)^2))
RMSEtestmod3 <- sqrt(mean((predict(rfMod, newdata = test_NCV) - test_NCV$pctfor)^2))
#############################
# predict(rfMod, newdata = NCV)
@
\vspace{-2ex}
\vfill
\end{block}
\vfill

%-- Block 3-2
\begin{block}{Prediction Errors}
<<Results, echo = FALSE, results='asis'>>=
x1 <- c(RMSEmod1a,  RMSEmod1b, RMSEmod2, RMSEmod3)
x2 <- c(RMSEtestmod1a, RMSEtestmod1b, RMSEtestmod2, RMSEtestmod3)
XR <- rbind(x1, x2)
dimnames(XR) <- list(c("Training Error", "Testing Error"), c('mod1A', 'mod1B', 'mod2', 'mod3'))
xtable(XR, align = rep("c", 5), caption ="Training and Testing Error are the RMSE computed from a $K=10$ cross-validated model for all models except mod3.  The RMSE for the random forest model (mod3) does not use cross-validation.", label ="PT")
@
\vspace{0ex}
\vfill
\end{block}
\vfill

%-- Block 3-3
\begin{block}{Further Directions}
\begin{itemize}
\item Use the models developed in this poster to predict county votes for states that have had similar marriage amendments such as South Carolina, Wisconsin, South Dakota, Florida, Idaho, Alabama, Utah, Michigan, Texas, Arkansas, Louisiana, Kansas, Kentucky, Ohio, and Nebraska. 
\item  Use ensemble methods (combining multiple models) for better prediction.
\item  Make our local maps available via the internet using the shiny server.
\end{itemize}
\vspace{0ex}
\vfill
\end{block}
\vfill

%-- Block 3-4
\begin{block}{References}
\footnotesize
\setbeamertemplate{bibliography item}[text]
\vspace{-1ex}
<<auto-bib, version=packageVersion('knitr'), echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE>>=
# write all packages in the current session to a bib file
write_bib(c(.packages()), file = 'knitr-packages.bib')
@

\bibliographystyle{plain}  % can use plain but comment out natbib at top if using plain
\bibliography{knitr-packages,poster}
\normalsize
\vfill
\end{block} 
\vfill

\end{minipage}
\end{column}%3




\end{columns}
\end{frame}
\end{document}

